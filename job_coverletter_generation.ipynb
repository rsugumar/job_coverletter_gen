{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef645663-d810-4574-aec5-a691c451fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF file\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "my_cv_path = \"/Users/rsukumar/Downloads/Sukumar_RAGHAVAN_CV.pdf\" \n",
    "\n",
    "loader = PyMuPDFLoader(my_cv_path)\n",
    "pdf_loaded_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962f5df4-5173-423e-b8bc-848833e3744a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d090bd95-53f5-4a64-9b85-6eab13d5d746',\n",
       " 'e6a7347a-971d-4163-b66e-0deaee562c02',\n",
       " '6268b12a-45cd-4bee-930e-9c773e0836ab']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store vector embeddings into the vector db for later use\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import create_langchain_embedding\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Using Huggingface embeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "langchain_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Using Ollama embeddings\n",
    "# from langchain_ollama import OllamaEmbeddings\n",
    "# langchain_embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "ef = create_langchain_embedding(langchain_embeddings)\n",
    "\n",
    "# Keeping option to switch between Ephemeral / Persistent chroma client, if wanted\n",
    "chroma_db_persist_path = \"chroma-db-job-coverletter\"\n",
    "vector_db_collection_name = \"job-cv-vector-db-collection\"\n",
    "\n",
    "# ephemeral_chroma_client = chromadb.EphemeralClient()\n",
    "persistent_chroma_client = chromadb.PersistentClient(path=chroma_db_persist_path)\n",
    "\n",
    "vector_db_client = persistent_chroma_client\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "chunked_docs = text_splitter.split_documents(pdf_loaded_data)\n",
    "\n",
    "vector_store_from_client = Chroma(\n",
    "    client=vector_db_client,\n",
    "    collection_name=vector_db_collection_name,\n",
    "    embedding_function=ef,\n",
    ")\n",
    "\n",
    "vector_store_from_client.add_documents(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b904603-063c-444a-9a4b-dd8b9fc6a3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rsukumar/my_workspace/job_coverletter_gen/.venv/lib/python3.12/site-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>The document appears to be the professional summary of Sukumar Raghavan, a Machine Learning Engineer, detailing his experience in developing and deploying cutting-edge AI solutions, including ContagionNET, which detects viral load of COVID-19 using computer vision pipelines and deep learning models.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# validating the created index\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from chromadb.utils.embedding_functions import create_langchain_embedding\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chroma_db_persist_path = \"chroma-db-job-coverletter\"\n",
    "vector_db_collection_name = \"job-cv-vector-db-collection\"\n",
    "vector_store_client = chromadb.PersistentClient(path=chroma_db_persist_path)\n",
    "langchain_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "ef = create_langchain_embedding(langchain_embeddings)\n",
    "\n",
    "vector_store_from_client = Chroma(\n",
    "    client=vector_store_client,\n",
    "    collection_name=vector_db_collection_name,\n",
    "    embedding_function=ef,\n",
    ")\n",
    "\n",
    "llm_instance = ChatOllama(model=\"llama3.2\", temperature=0.7)\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vector_store_from_client.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_instance\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"What is the document all about?\"\n",
    "response = qa_chain.invoke(query)\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c081fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def my_display(display_text):\n",
    "  display(Markdown(f\"<b>{display_text}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2236fcc6-f422-4461-8886-a8bc10e1f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# addtional context enrichment input using the job description\n",
    "job_description = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc9f610-dcc6-43eb-b772-24efef14d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from chromadb.utils.embedding_functions import create_langchain_embedding\n",
    "\n",
    "def preprocess_job_description(job_description):\n",
    "    \"\"\"Extract key requirements and skills from job description\"\"\"\n",
    "    # You could use an LLM to extract key requirements\n",
    "    extraction_prompt = \"\"\"\n",
    "    Analyze the following job description and extract:\n",
    "    1. Required technical skills\n",
    "    2. Required soft skills\n",
    "    3. Main responsibilities\n",
    "    4. Key technologies mentioned\n",
    "    \n",
    "    Job Description:\n",
    "    {job_description}\n",
    "    \n",
    "    Provide a structured summary of the key requirements.\n",
    "    \"\"\"\n",
    "\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    requirements = llm.invoke(extraction_prompt.format(job_description=job_description))\n",
    "\n",
    "    return requirements\n",
    "\n",
    "\n",
    "def get_relevant_experience(vector_db, job_description):\n",
    "    # First, extract key requirements from job description\n",
    "    job_query = f\"Find experience and skills related to: {job_description}\"\n",
    "\n",
    "    # Get relevant experience matching job requirements\n",
    "    relevant_docs = vector_db.similarity_search(\n",
    "        job_query,\n",
    "        k=4,  # Increase number of relevant chunks\n",
    "    )\n",
    "\n",
    "    # Get general professional summary\n",
    "    summary_docs = vector_db.similarity_search(\n",
    "        \"professional summary and key achievements\", k=2\n",
    "    )\n",
    "\n",
    "    return relevant_docs + summary_docs\n",
    "\n",
    "\n",
    "def build_context(retrieved_docs, job_description):\n",
    "    cv_context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    return {\n",
    "        \"context\": cv_context,\n",
    "        \"job_description\": job_description,\n",
    "        \"query\": \"Write a tailored cover letter for this specific job position.\",\n",
    "    }\n",
    "\n",
    "# Generate cover letter\n",
    "# Use in main flow\n",
    "def generate_tailored_cover_letter(vector_store_client, job_description):\n",
    "    # Preprocess job description to extract key requirements\n",
    "    key_requirements = preprocess_job_description(job_description)\n",
    "\n",
    "    lc_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    ef = create_langchain_embedding(lc_embeddings)\n",
    "    vector_db = Chroma(\n",
    "        client=vector_store_client,\n",
    "        collection_name=vector_db_collection_name,\n",
    "        embedding_function=ef,\n",
    "    )\n",
    "\n",
    "    # Use these requirements to guide relevant experience retrieval\n",
    "    relevant_docs = get_relevant_experience(vector_db, key_requirements)\n",
    "\n",
    "    # Generate cover letter with explicit focus on matching requirements\n",
    "    input_context = build_context(relevant_docs, job_description)\n",
    "\n",
    "    # Add to the chain configuration\n",
    "    chain_config = {\n",
    "        \"document_separator\": \"\\n\\n\",\n",
    "        \"max_tokens_limit\": 2000,\n",
    "        \"return_source_documents\": True,\n",
    "        \"token_overlap\": 200,\n",
    "    }\n",
    "\n",
    "    # Adjust LLM parameters\n",
    "    llm_config = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"presence_penalty\": 0.6,\n",
    "        \"frequency_penalty\": 0.6,\n",
    "    }\n",
    "\n",
    "    # Use local Ollama model to generate the cover letter.\n",
    "    # llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", **llm_config)\n",
    "    llm = ChatOllama(model=\"llama3.2\", **llm_config)\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 4},\n",
    "            search_type=\"mmr\",  # Use Maximum Marginal Relevance for better diversity\n",
    "            **chain_config)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Prompt template\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer just say \"I don't know\", don't try to make up an answer.\n",
    "    Context: {context}\n",
    "\n",
    "    Job Description: {job_description}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    qa_chain = (\n",
    "        {\n",
    "            \"context\": itemgetter('retrieved_context') | retriever | format_docs,\n",
    "            \"query\": itemgetter('query'),\n",
    "            \"job_description\": itemgetter('job_description')\n",
    "        }\n",
    "        | QA_CHAIN_PROMPT\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # query\n",
    "    question = \"Write a cover letter for me to apply for the job described in the given job description using the given context.\"\n",
    "\n",
    "    response = qa_chain.invoke({\n",
    "        \"retrieved_context\": input_context['context'],\n",
    "        \"job_description\": input_context['job_description'],\n",
    "        \"query\": question,\n",
    "    })\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbc8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "\n",
    "import chromadb\n",
    "\n",
    "chroma_db_persist_path = \"chroma-db-job-coverletter\"\n",
    "vector_db_collection_name = \"job-cv-vector-db-collection\"\n",
    "\n",
    "persistent_chroma_client = chromadb.PersistentClient(path=chroma_db_persist_path)\n",
    "\n",
    "generated_cover_letter = generate_tailored_cover_letter(persistent_chroma_client, job_description)\n",
    "\n",
    "my_display(generated_cover_letter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "/Users/rsukumar/my_workspace/job_coverletter_gen",
   "language": "python",
   "name": "_users_rsukumar_my_workspace_job_coverletter_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
