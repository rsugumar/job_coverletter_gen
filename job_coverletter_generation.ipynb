{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef645663-d810-4574-aec5-a691c451fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF file\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "my_cv_path = \"/Users/rsukumar/Downloads/Sukumar_RAGHAVAN_CV.pdf\"\n",
    "\n",
    "loader = PyMuPDFLoader(my_cv_path)\n",
    "pdf_loaded_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfac1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "import chromadb\n",
    "\n",
    "# Adjust LLM parameters\n",
    "llm_config = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"presence_penalty\": 0.6,\n",
    "    \"frequency_penalty\": 0.6,\n",
    "}\n",
    "\n",
    "# Adjust Vector client\n",
    "chroma_db_persist_path = \"chroma-db-job-coverletter\"\n",
    "vector_db_collection_name = \"job-cv-vector-db-collection\"\n",
    "# Keeping option to switch between Ephemeral / Persistent chroma client, if wanted\n",
    "# ephemeral_chroma_client = chromadb.EphemeralClient()\n",
    "persistent_chroma_client = chromadb.PersistentClient(path=chroma_db_persist_path)\n",
    "\n",
    "model_config = {\n",
    "    \"langchain_embeddings\": HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    # \"langchain_embeddings\": OllamaEmbeddings(model=\"llama3.2\"),\n",
    "    \"llm_instance\": ChatOllama(model=\"llama3.2\", config=llm_config),\n",
    "    \"vector_db_client\": persistent_chroma_client,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "962f5df4-5173-423e-b8bc-848833e3744a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['75be8fb0-6c6a-4c16-bb4d-400c475bf8b5',\n",
       " '96a4fb5f-54d8-4900-ba22-fae27051587c',\n",
       " '893d5cf2-77a0-4f71-b120-41f4aca47681']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store vector embeddings into the vector db for later use\n",
    "\n",
    "from chromadb.utils.embedding_functions import create_langchain_embedding\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "ef = create_langchain_embedding(model_config[\"langchain_embeddings\"])\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "chunked_docs = text_splitter.split_documents(pdf_loaded_data)\n",
    "\n",
    "model_config[\"vector_db_client\"].delete_collection(vector_db_collection_name)\n",
    "model_config[\"vector_db_client\"].create_collection(\n",
    "    vector_db_collection_name, embedding_function=ef\n",
    ")\n",
    "\n",
    "vector_store_from_client = Chroma(\n",
    "    client=model_config[\"vector_db_client\"],\n",
    "    collection_name=vector_db_collection_name,\n",
    "    embedding_function=ef,\n",
    ")\n",
    "\n",
    "vector_store_from_client.add_documents(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c081fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def my_display(display_text):\n",
    "    display(Markdown(f\"<b>{display_text}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b904603-063c-444a-9a4b-dd8b9fc6a3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rsukumar/my_workspace/job_coverletter_gen/.venv/lib/python3.12/site-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>The document speaks about Sukumar Raghavan, a seasoned Machine Learning Engineer with 6+ years of experience in AI and over a decade of software development experience.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# validating the created index\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from chromadb.utils.embedding_functions import create_langchain_embedding\n",
    "\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "ef = create_langchain_embedding(model_config[\"langchain_embeddings\"])\n",
    "\n",
    "vector_db = Chroma(\n",
    "    client=model_config[\"vector_db_client\"],\n",
    "    collection_name=vector_db_collection_name,\n",
    "    embedding_function=ef,\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": vector_db.as_retriever() | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model_config[\"llm_instance\"]\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"About who this document speaks?\"\n",
    "response = qa_chain.invoke(query)\n",
    "\n",
    "my_display(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2236fcc6-f422-4461-8886-a8bc10e1f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# addtional context enrichment input using the job description\n",
    "job_description = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbc9f610-dcc6-43eb-b772-24efef14d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from chromadb.utils.embedding_functions import create_langchain_embedding\n",
    "\n",
    "\n",
    "def preprocess_job_description(job_description):\n",
    "    \"\"\"Extract key requirements and skills from job description\"\"\"\n",
    "    # You could use an LLM to extract key requirements\n",
    "    extraction_prompt = \"\"\"\n",
    "    Analyze the following job description and extract:\n",
    "    1. Required technical skills\n",
    "    2. Required soft skills\n",
    "    3. Main responsibilities\n",
    "    4. Key technologies mentioned\n",
    "    \n",
    "    Job Description:\n",
    "    {job_description}\n",
    "    \n",
    "    Provide a structured summary of the key requirements.\n",
    "    \"\"\"\n",
    "\n",
    "    llm = model_config[\"llm_instance\"]\n",
    "    requirements = llm.invoke(extraction_prompt.format(job_description=job_description))\n",
    "\n",
    "    return requirements\n",
    "\n",
    "\n",
    "def get_relevant_experience(vector_db, job_description):\n",
    "    # First, extract key requirements from job description\n",
    "    job_query = f\"Find experience and skills related to: {job_description}\"\n",
    "\n",
    "    # Get relevant experience matching job requirements\n",
    "    relevant_docs = vector_db.similarity_search(\n",
    "        job_query,\n",
    "        k=4,  # Increase number of relevant chunks\n",
    "    )\n",
    "\n",
    "    # Get general professional summary\n",
    "    summary_docs = vector_db.similarity_search(\n",
    "        \"professional summary and key achievements\", k=2\n",
    "    )\n",
    "\n",
    "    return relevant_docs + summary_docs\n",
    "\n",
    "\n",
    "def build_context(retrieved_docs, job_description):\n",
    "    cv_context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    return {\n",
    "        \"context\": cv_context,\n",
    "        \"job_description\": job_description,\n",
    "        \"query\": \"Write a tailored cover letter for this specific job position as a job applicant.\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Generate cover letter\n",
    "# Use in main flow\n",
    "def generate_tailored_cover_letter(vector_store_client, job_description):\n",
    "    # Preprocess job description to extract key requirements\n",
    "    key_requirements = preprocess_job_description(job_description)\n",
    "    ef = create_langchain_embedding(model_config[\"langchain_embeddings\"])\n",
    "\n",
    "    vector_db = Chroma(\n",
    "        client=vector_store_client,\n",
    "        collection_name=vector_db_collection_name,\n",
    "        embedding_function=ef,\n",
    "    )\n",
    "\n",
    "    # Use these requirements to guide relevant experience retrieval\n",
    "    relevant_docs = get_relevant_experience(vector_db, key_requirements)\n",
    "\n",
    "    # Generate cover letter with explicit focus on matching requirements\n",
    "    input_context = build_context(relevant_docs, job_description)\n",
    "\n",
    "    # Add to the chain configuration\n",
    "    chain_config = {\n",
    "        \"document_separator\": \"\\n\\n\",\n",
    "        \"max_tokens_limit\": 2000,\n",
    "        \"return_source_documents\": True,\n",
    "        \"token_overlap\": 200,\n",
    "    }\n",
    "\n",
    "    # Use local Ollama model to generate the cover letter.\n",
    "    # llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", **llm_config)\n",
    "    llm = model_config[\"llm_instance\"]\n",
    "    retriever = vector_db.as_retriever(\n",
    "        search_kwargs={\"k\": 4},\n",
    "        search_type=\"mmr\",  # Use Maximum Marginal Relevance for better diversity\n",
    "        **chain_config,\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Prompt template\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer just say \"I don't know\", don't try to make up an answer.\n",
    "    Context: {context}\n",
    "\n",
    "    Job Description: {job_description}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    qa_chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"retrieved_context\") | retriever | format_docs,\n",
    "            \"query\": itemgetter(\"query\"),\n",
    "            \"job_description\": itemgetter(\"job_description\"),\n",
    "        }\n",
    "        | QA_CHAIN_PROMPT\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    response = qa_chain.invoke(\n",
    "        {\n",
    "            \"retrieved_context\": input_context[\"context\"],\n",
    "            \"job_description\": input_context[\"job_description\"],\n",
    "            \"query\": input_context[\"query\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %debug\n",
    "generated_cover_letter = generate_tailored_cover_letter(\n",
    "    model_config[\"vector_db_client\"], job_description\n",
    ")\n",
    "\n",
    "my_display(generated_cover_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a917be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "Number of requested results 20 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def gradio_interface_to_generate_cover_letter(job_description):\n",
    "    return generate_tailored_cover_letter(\n",
    "        model_config[\"vector_db_client\"], job_description\n",
    "    )\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    # Tailored job cover letter generation using AI (Llama)!\n",
    "    Submit a job description to generate a tailored cover letter for the job position.\n",
    "    \"\"\"\n",
    "    )\n",
    "    inp = gr.Textbox(\n",
    "        label=\"Job Description\", placeholder=\"Copy & Paste the job description here\"\n",
    "    )\n",
    "    out = gr.Textbox(\n",
    "        label=\"AI Generated Cover Letter\",\n",
    "        placeholder=\"AI generated cover letter will be here\",\n",
    "    )\n",
    "    submit_btn = gr.Button(\"Generate Cover Letter Now!\")\n",
    "    submit_btn.click(\n",
    "        fn=gradio_interface_to_generate_cover_letter,\n",
    "        inputs=inp,\n",
    "        outputs=out,\n",
    "        api_name=\"gradio_interface_to_generate_cover_letter\",\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f41b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "/Users/rsukumar/my_workspace/job_coverletter_gen",
   "language": "python",
   "name": "_users_rsukumar_my_workspace_job_coverletter_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
