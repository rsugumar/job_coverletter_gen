{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef645663-d810-4574-aec5-a691c451fe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported LlamaIndex\n",
      "Processing /Users/rsukumar/Downloads/Sukumar_RAGHAVAN_CV.pdf...\n",
      "]\n",
      "Processing /Users/rsukumar/Downloads/Sukumar_RAGHAVAN_CV.pdf...\n",
      "]========================================] (1/1)\n"
     ]
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "my_cv_path = \"/Users/rsukumar/Downloads/Sukumar_RAGHAVAN_CV.pdf\"\n",
    "\n",
    "# Load pdf file\n",
    "llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
    "llama_docs = llama_reader.load_data(my_cv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "962f5df4-5173-423e-b8bc-848833e3744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/rsukumar/Library/Caches/llama_index/models/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.34 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   170.20 MiB, ( 1101.68 /  4080.00)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "llm_load_tensors:      Metal buffer size =   170.20 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: AMD Radeon Pro 5300M\n",
      "ggml_metal_init: found device: Intel(R) UHD Graphics 630\n",
      "ggml_metal_init: picking default device: AMD Radeon Pro 5300M\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   AMD Radeon Pro 5300M\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = false\n",
      "ggml_metal_init: hasUnifiedMemory              = false\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  4278.19 MB\n",
      "ggml_metal_init: skipping kernel_mul_mm_f32_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_f16_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q8_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q2_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q3_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q6_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_m_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_nl_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f32_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f16_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q8_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q2_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q3_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q6_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_m_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_nl_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h64            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h80            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h96            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h112           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h128           (not supported)\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2973.75 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =    76.25 MiB\n",
      "llama_new_context_with_model: KV self size  = 3050.00 MiB, K (f16): 1525.00 MiB, V (f16): 1525.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   352.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 627\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '4096', 'llama.attention.head_count': '40', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: llama-2\n",
      "/Users/rsukumar/my_workspace/job_coverletter_gen2/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:1138: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "\n",
      "llama_print_timings:        load time =   63955.26 ms\n",
      "llama_print_timings:      sample time =       4.54 ms /   122 runs   (    0.04 ms per token, 26866.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   83161.50 ms /   625 tokens (  133.06 ms per token,     7.52 tokens per second)\n",
      "llama_print_timings:        eval time =   46888.50 ms /   121 runs   (  387.51 ms per token,     2.58 tokens per second)\n",
      "llama_print_timings:       total time =  130160.99 ms /   746 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>  Based on the provided context information, the document appears to be a curriculum vitae (CV) or a resume of a person named Sukumar Raghavan. The document contains information about his educational background, work experience, and skills. The document is in PDF format, and it was created on December 11, 2024, at 5:32 AM. The document has 2 pages, and the file path is /Users/rsukumar/Downloads/Sukumar_RAGHAVAN_CV.pdf.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import create_langchain_embedding\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# Using Huggingface embeddings\n",
    "langchain_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "ef = create_langchain_embedding(langchain_embeddings)\n",
    "embed_model = LangchainEmbedding(langchain_embeddings)\n",
    "\n",
    "# Keeping option to switch between Ephemeral / Persistent chroma client\n",
    "chroma_persist_path = \"chroma-db-job-coverletter\"\n",
    "vector_db_collection_name = \"job-cv-vector-db-collection\"\n",
    "# ephemeral_chroma_client = chromadb.EphemeralClient()\n",
    "persistent_chroma_client = chromadb.PersistentClient(path=chroma_persist_path)\n",
    "vector_db_client = persistent_chroma_client\n",
    "\n",
    "chroma_collection = vector_db_client.get_or_create_collection(\n",
    "    vector_db_collection_name, embedding_function=ef\n",
    ")\n",
    "\n",
    "vector_store_from_client = Chroma(\n",
    "    client=vector_db_client,\n",
    "    collection_name=vector_db_collection_name,\n",
    "    embedding_function=ef,\n",
    ")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    llama_docs,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "# Query Data\n",
    "query_engine = index.as_query_engine(llm=\"local\", streaming=True)\n",
    "response = query_engine.query(\"What is the document all about?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf7eb54-30aa-48b3-80e2-372c5bc70c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g1/7p_ff_xn2ms3b005zgc5tbrc0000gn/T/ipykernel_79326/1785563842.py:35: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>I'd be happy to help with that. Here's a sample cover letter:\n",
       "\n",
       "[Your Name]\n",
       "[Address]\n",
       "[City, State ZIP Code]\n",
       "[Date]\n",
       "\n",
       "[Hiring Manager's Name]\n",
       "[Company Name]\n",
       "[Address]\n",
       "[City, State ZIP Code]\n",
       "\n",
       "Dear [Hiring Manager's Name],\n",
       "\n",
       "I am excited to apply for the [Job Title] position at [Company Name]. With [Number of Years] years of experience in [Industry/Field], I am confident that my skills and qualifications make me an ideal candidate for this role.\n",
       "\n",
       "As a highly motivated and dedicated professional, I have a proven track record of [Desirable Skill or Achievement]. In my current role at [Current Company], I have successfully [Briefly describe a relevant accomplishment or experience]. I am excited about the opportunity to bring my skills and experience to a new challenge and contribute to the success of [Company Name].\n",
       "\n",
       "I am particularly drawn to this role because of [Reason for interest in company or position]. I am impressed by [Company's Mission or Values] and believe that my own values and work style align with those of the organization.\n",
       "\n",
       "Thank you for considering my application. I would welcome the opportunity to discuss this position further and explain in greater detail why I am the ideal candidate for this role. Please feel free to contact me at [Your Contact Information].\n",
       "\n",
       "Sincerely,\n",
       "\n",
       "[Your Name]\n",
       "\n",
       "Is there anything else I can help you with?</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Use local Ollama model to answer the question\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "# Prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer just say I don't know, don't try to make up an answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer as a jobseeker:\"\"\"\n",
    "\n",
    "question = \"Write a cover letter for applying to the job.\"\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "chroma_db_persist_path = \"chroma-db-job-coverletter\"\n",
    "vector_db_collection_name = \"job-cv-vector-db-collection\"\n",
    "lc_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "ef2 = create_langchain_embedding(lc_embeddings)\n",
    "\n",
    "vector_db = Chroma(\n",
    "    collection_name=\"sukumar-cv-vector-db-collection\",\n",
    "    persist_directory=chroma_db_persist_path,\n",
    "    embedding_function=ef2,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vector_db.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    # chain_type=\"map_reduce\", # other techniques to experiment: \"refine\", \"map_rerank\"\n",
    ")\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "display(Markdown(f\"<b>{result[\"result\"]}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5a808-ea5c-42f7-9d4c-6b603db1661a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "/Users/rsukumar/my_workspace/job_coverletter_gen2",
   "language": "python",
   "name": "_users_rsukumar_my_workspace_job_coverletter_gen2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
