{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef645663-d810-4574-aec5-a691c451fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF file\n",
    "\n",
    "import pymupdf4llm\n",
    "\n",
    "my_cv_path = \"/Users/rsukumar/Downloads/Sukumar_RAGHAVAN_CV.pdf\"\n",
    "\n",
    "llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
    "llama_docs = llama_reader.load_data(my_cv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962f5df4-5173-423e-b8bc-848833e3744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store vector embeddings into the vector db for later use\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import create_langchain_embedding\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# Using Huggingface embeddings\n",
    "# langchain_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Using Ollama embeddings\n",
    "langchain_embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "ef = create_langchain_embedding(langchain_embeddings)\n",
    "embed_model = LangchainEmbedding(langchain_embeddings)\n",
    "\n",
    "# Keeping option to switch between Ephemeral / Persistent chroma client, if wanted\n",
    "chroma_db_persist_path = \"chroma-db-job-coverletter\"\n",
    "vector_db_collection_name = \"job-cv-vector-db-collection\"\n",
    "\n",
    "# ephemeral_chroma_client = chromadb.EphemeralClient()\n",
    "persistent_chroma_client = chromadb.PersistentClient(path=chroma_db_persist_path)\n",
    "vector_db_client = persistent_chroma_client\n",
    "\n",
    "chroma_collection = vector_db_client.get_or_create_collection(\n",
    "    vector_db_collection_name, embedding_function=ef\n",
    ")\n",
    "\n",
    "vector_store_from_client = Chroma(\n",
    "    client=vector_db_client,\n",
    "    collection_name=vector_db_collection_name,\n",
    "    embedding_function=ef,\n",
    ")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    llama_docs,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b904603-063c-444a-9a4b-dd8b9fc6a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating the index\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "llm_instance = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "# Query Data\n",
    "query_engine = index.as_query_engine(llm=llm_instance, streaming=True)\n",
    "response = query_engine.query(\"What is the document all about?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2236fcc6-f422-4461-8886-a8bc10e1f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context enrichment input\n",
    "add_job_description_context = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc9f610-dcc6-43eb-b772-24efef14d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_job_description(job_description):\n",
    "    \"\"\"Extract key requirements and skills from job description\"\"\"\n",
    "    # You could use an LLM to extract key requirements\n",
    "    extraction_prompt = \"\"\"\n",
    "    Analyze the following job description and extract:\n",
    "    1. Required technical skills\n",
    "    2. Required soft skills\n",
    "    3. Main responsibilities\n",
    "    4. Key technologies mentioned\n",
    "    \n",
    "    Job Description:\n",
    "    {job_description}\n",
    "    \n",
    "    Provide a structured summary of the key requirements.\n",
    "    \"\"\"\n",
    "\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    requirements = llm.invoke(extraction_prompt.format(job_description=job_description))\n",
    "\n",
    "    return requirements\n",
    "\n",
    "\n",
    "def get_relevant_experience(vector_db, job_description):\n",
    "    # First, extract key requirements from job description\n",
    "    job_query = f\"Find experience and skills related to: {job_description}\"\n",
    "\n",
    "    # Get relevant experience matching job requirements\n",
    "    relevant_docs = vector_db.similarity_search(\n",
    "        job_query,\n",
    "        k=4,  # Increase number of relevant chunks\n",
    "        # search_type=\"similarity\"\n",
    "    )\n",
    "\n",
    "    # Get general professional summary\n",
    "    summary_docs = vector_db.similarity_search(\n",
    "        \"professional summary and key achievements\", k=2\n",
    "    )\n",
    "\n",
    "    return relevant_docs + summary_docs\n",
    "\n",
    "\n",
    "def build_context(retrieved_docs, job_description):\n",
    "    cv_context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    return {\n",
    "        \"context\": cv_context,\n",
    "        \"job_description\": job_description,\n",
    "        \"query\": \"Write a tailored cover letter for this specific job position.\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Generate cover letter\n",
    "# Use in main flow\n",
    "def generate_tailored_cover_letter(vector_db, job_description):\n",
    "    # Preprocess job description to extract key requirements\n",
    "    key_requirements = preprocess_job_description(job_description)\n",
    "\n",
    "    # Use these requirements to guide relevant experience retrieval\n",
    "    relevant_docs = get_relevant_experience(vector_db, key_requirements)\n",
    "\n",
    "    # Generate cover letter with explicit focus on matching requirements\n",
    "    input_context = build_context(relevant_docs, job_description)\n",
    "    result = qa_chain.invoke(input_context)\n",
    "\n",
    "    return result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7eb54-30aa-48b3-80e2-372c5bc70c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main flow\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from chromadb.utils.embedding_functions import create_langchain_embedding\n",
    "from IPython.display import Markdown, display\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer just say \"I don't know\", don't try to make up an answer.\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer as a jobseeker:\"\"\"\n",
    "\n",
    "# query\n",
    "question = \"Write a cover letter for applying for the job role as described in the context matching the job description exactly in the additional context.\"\n",
    "\n",
    "# Add to the chain configuration\n",
    "chain_config = {\n",
    "    \"document_separator\": \"\\n\\n\",\n",
    "    \"max_tokens_limit\": 2000,\n",
    "    \"return_source_documents\": True,\n",
    "    \"token_overlap\": 200,\n",
    "}\n",
    "\n",
    "# Adjust LLM parameters\n",
    "llm_config = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"presence_penalty\": 0.6,\n",
    "    \"frequency_penalty\": 0.6,\n",
    "}\n",
    "\n",
    "# Use local Ollama model to generate the cover letter.\n",
    "llm = ChatOllama(model=\"llama3.2\", **llm_config)\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"job_description\", \"query\"]\n",
    ")\n",
    "\n",
    "chroma_db_persist_path = \"chroma-db-job-coverletter\"\n",
    "vector_db_collection_name = \"job-cv-vector-db-collection\"\n",
    "\n",
    "lc_embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "# lc_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "ef2 = create_langchain_embedding(lc_embeddings)\n",
    "\n",
    "vector_db = Chroma(\n",
    "    collection_name=vector_db_collection_name,\n",
    "    persist_directory=chroma_db_persist_path,\n",
    "    embedding_function=ef2,\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vector_db.as_retriever(\n",
    "        search_kwargs={\"k\": 4},\n",
    "        search_type=\"mmr\",  # Use Maximum Marginal Relevance for better diversity\n",
    "        **chain_config,\n",
    "    ),\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": QA_CHAIN_PROMPT,\n",
    "        \"verbose\": True,  # Helpful for debugging\n",
    "    },\n",
    "    chain_type=\"stuff\",  # other techniques to experiment: \"map_reduce\", \"refine\", \"map_rerank\"\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "cover_letter = generate_tailored_cover_letter(vector_db, add_job_description_context)\n",
    "display(Markdown(f\"<b>{cover_letter}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342f978-f157-45fe-9766-95e6022ff5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "/Users/rsukumar/my_workspace/job_coverletter_gen",
   "language": "python",
   "name": "_users_rsukumar_my_workspace_job_coverletter_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
